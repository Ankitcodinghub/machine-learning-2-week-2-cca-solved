# machine-learning-2-week-2-cca-solved
**TO GET THIS SOLUTION VISIT:** [Machine Learning 2 Week 2-CCA Solved](https://www.ankitcodinghub.com/product/machine-learning-2-week-2-cca-solved/)


---

üì© **If you need this solution or have special requests:** **Email:** ankitcoding@gmail.com  
üì± **WhatsApp:** +1 419 877 7882  
üìÑ **Get a quote instantly using this form:** [Ask Homework Questions](https://www.ankitcodinghub.com/services/ask-homework-questions/)

*We deliver fast, professional, and affordable academic help.*

---

<h2>Description</h2>



<div class="kk-star-ratings kksr-auto kksr-align-center kksr-valign-top" data-payload="{&quot;align&quot;:&quot;center&quot;,&quot;id&quot;:&quot;98817&quot;,&quot;slug&quot;:&quot;default&quot;,&quot;valign&quot;:&quot;top&quot;,&quot;ignore&quot;:&quot;&quot;,&quot;reference&quot;:&quot;auto&quot;,&quot;class&quot;:&quot;&quot;,&quot;count&quot;:&quot;0&quot;,&quot;legendonly&quot;:&quot;&quot;,&quot;readonly&quot;:&quot;&quot;,&quot;score&quot;:&quot;0&quot;,&quot;starsonly&quot;:&quot;&quot;,&quot;best&quot;:&quot;5&quot;,&quot;gap&quot;:&quot;4&quot;,&quot;greet&quot;:&quot;Rate this product&quot;,&quot;legend&quot;:&quot;0\/5 - (0 votes)&quot;,&quot;size&quot;:&quot;24&quot;,&quot;title&quot;:&quot;Machine Learning 2 Week 2-CCA Solved&quot;,&quot;width&quot;:&quot;0&quot;,&quot;_legend&quot;:&quot;{score}\/{best} - ({count} {votes})&quot;,&quot;font_factor&quot;:&quot;1.25&quot;}">

<div class="kksr-stars">

<div class="kksr-stars-inactive">
            <div class="kksr-star" data-star="1" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="2" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="3" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="4" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="5" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>

<div class="kksr-stars-active" style="width: 0px;">
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>
</div>


<div class="kksr-legend" style="font-size: 19.2px;">
            <span class="kksr-muted">Rate this product</span>
    </div>
    </div>
<div class="page" title="Page 1">
<div class="layoutArea">
<div class="column"></div>
<div class="column">
&nbsp;

Exercise Sheet 2

</div>
</div>
<div class="layoutArea">
<div class="column">
Recall: For a sample of d1- and d2-dimensional data of size N, given as two data matrices X ‚àà Rd1√óN, Y ‚àà Rd2√óN (assumed to be centered), canonical correlation analysis (CCA) finds a one-dimensional projection maximizing the cross-correlation for constant auto-correlation. The primal optimization problem is:

Find wx ‚àà Rd1 , wy ‚àà Rd2 maximizing wx‚ä§ Cxy wy

subject to wx‚ä§Cxxwx = 1 (1)

w y‚ä§ C y y w y = 1 ,

where Cxx = N1 XX‚ä§ ‚àà Rd1√ód1 and Cyy = N1 Y Y ‚ä§ ‚àà Rd2√ód2 are the auto-covariance matrices of X resp. Y ,

and Cxy = N1 XY ‚ä§ ‚àà Rd1√ód2 is the cross-covariance matrix of X and Y . Exercise 1: Primal CCA (10 + 5 P)

We have seen in the lecture that a solution of the canonical correlation analysis can be found in some eigenvector of the generalized eigenvalue problem:

</div>
</div>
<div class="layoutArea">
<div class="column">
Ùè∞Ö 0 CxyÙè∞ÜÙè∞ÖwxÙè∞Ü Ùè∞ÖCxx 0 Ùè∞ÜÙè∞ÖwxÙè∞Ü C0w=Œª0Cw

</div>
</div>
<div class="layoutArea">
<div class="column">
yx y yyy

</div>
</div>
<div class="layoutArea">
<div class="column">
<ol>
<li>(a) &nbsp;Show that among all eigenvectors (wx,wy) the solution is the one associated to the highest eigenvalue.</li>
<li>(b) &nbsp;Show that if (wx,wy) is a solution, then (‚àíwx,‚àíwy) is also a solution of the CCA problem.Exercise 2: Dual CCA (10+15+5+5 P)In this exercise, we would like to derive the dual optimization problem.</li>
</ol>
<ol>
<li>(a) &nbsp;Show, that it is always possible to find an optimal solution in the span of the data, that is,wx=XŒ±x, wy=YŒ±y with some coefficient vectors Œ±x ‚àà RN and Œ±y ‚àà RN .</li>
<li>(b) &nbsp;Show that the solution of the dual optimization problem is found in an eigenvector of the generalized eigenvalue problem</li>
</ol>
Ùè∞Ö 0 A¬∑BÙè∞ÜÙè∞ÖŒ±xÙè∞Ü Ùè∞ÖA2 0 Ùè∞ÜÙè∞ÖŒ±xÙè∞Ü B¬∑A 0 Œ± =œÅ¬∑ 0 B2 Œ±

</div>
</div>
<div class="layoutArea">
<div class="column">
yy

</div>
</div>
<div class="layoutArea">
<div class="column">
where A = X‚ä§X and B = Y ‚ä§Y .

(c) Show that the solution of the dual is given by the eigenvector associated to the highest eigenvalue.

(d) Show how a solution to the original problem can be obtained from the solution of the generalized eigenvalue problem of the dual.

Exercise 3: CCA and Least Square Regression (20 P)

Consider some supervised dataset with the inputs stored in a matrix X ‚àà RD√óN and the targets stored in a vector y ‚àà RN . We assume that both our inputs and targets are centered. The least squares regression optimization problem is:

min ‚à•X‚ä§v ‚àí y‚à•2 v‚ààRD

We would like to relate least square regression and CCA, specifically, their respective solutions v and (wx, wy).

(a) Show that if X and y are the two modalities of CCA (i.e. X ‚àà RD√óN and y ‚àà R1√óN), the first part of the solution of CCA (i.e. the vector wx) is equivalent to the solution v of least square regression up to a scaling factor.

Exercise 4: Programming (30 P)

Download the programming files on ISIS and follow the instructions.

</div>
</div>
</div>
<div class="page" title="Page 2">
<div class="section">
<div class="layoutArea">
<div class="column">
Exercise sheet 2 (programming) [SoSe 2021] Machine Learning 2

</div>
</div>
<div class="layoutArea">
<div class="column">
Canonical Correlation Analysis

In this exercise, we consider canonical correlation analysis (CCA) on two simple problems, one in low dimensions and one in high dimensions. The goal is to implement the primal and dual versions of CCA to handle these two different cases. The first dataset consists of two trajectories in two dimensions. The dataset is extracted and plotted below. The first data points are shown in dark blue, and the last ones are shown in yellow.

In [1]:

import numpy

import matplotlib

%matplotlib inline

from matplotlib import pyplot as plt import utils

<pre>X,Y   = utils.getdata()
p1,p2 = utils.plotdata(X,Y)
</pre>
For these two trajetories, that can be understood as two different modalities of the same data, we would like determine under which projections they appear maximally correlated.

Implementing Primal CCA

As stated in the lecture, the CCA problem in its primal form consists of maximizing the cross-correlation objective: $$J(w_x,w_y) = w_x^\top C_{xy} w_y$$

subject to autcorrelation constraints $w_x^\top C_{xx} w_x = 1$ and $w_y^\top C_{yy} w_y = 1$. Using the method of Lagrange multipliers, this optimization problem can be reduced to finding the first eigenvector of the generalized eigenvalue problem:

$$ \begin{bmatrix}0 &amp; C_{xy}\\C_{yx} &amp; 0\end{bmatrix} \begin{bmatrix}w_x\\w_y\end{bmatrix} = \lambda \begin{bmatrix}C_{xx} &amp; 0\\0 &amp; C_{yy}\end{bmatrix} \begin{bmatrix}w_x\\w_y\end{bmatrix} $$

Your first task is to write a function that solves the CCA problem in the primal (i.e. that solves the generalized eigenvalue problem above). The function you needtoimplementreceivestwomatrices X and Y ofsize N $\times$ d1 and N $\times$ d2 respectively.Itreturnstwovectorsofsize d1 and

d2 correspondingtotheprojectionsassociatedtothemodalities X and Y.(Hint:Notethatthedatamatrices X and Y havenotbeencenteredyet.) In [2]:

import numpy

def CCAprimal(X,Y):

## ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî- ## TODO: replace by your solution ## ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî- import solution

<pre>    wx,wy = solution.CCAprimal(X,Y)
</pre>
<pre>    ## -------------------------------
</pre>
return wx,wy

The function can now be called with our dataset. The learned projection vectors $w_x$ and $w_y$ are plotted as red arrows.

</div>
</div>
</div>
</div>
<div class="page" title="Page 3">
<div class="section">
<div class="layoutArea">
<div class="column">
<pre>In [3]:
wx,wy = CCAprimal(X,Y)
</pre>
<pre>p1,p2 = utils.plotdata(X,Y)
p1.arrow(0,0,1*wx[0],1*wx[1],color='red',width=0.1)
p2.arrow(0,0,1*wy[0],1*wy[1],color='red',width=0.1)
plt.show()
</pre>
</div>
</div>
<div class="layoutArea">
<div class="column">
In each modality, the arrow points in a specific direction (note that the optimal CCA directions are defined up to a sign flip of both $w_x$ and $w_y$). Furthermore, we can verify CCA has learned a meaningful solution by projecting the data on it.

In [4]:

<pre>plt.figure(figsize=(6,2))
plt.plot(numpy.dot(X,wx))
plt.plot(numpy.dot(Y,wy))
plt.show()
</pre>
Clearly, the data is correlated in the projected space.

Implementing Dual CCA

Inthesecondpartoftheexercise,weconsiderthecasewherethedataishighdimensional(with d $\gg$ N).Suchhigh-dimensionalityoccursfor example, when input data are images. We consider the scenario where sources emit spatially, and two (noisy) receivers measure the spatial field at different locations. We would like to identify signal that is common to the two measured locations, e.g. a given source emitting at a given frequency. We first load the data and show one example.

<pre>In [5]:
X,Y = utils.getHDdata()
</pre>
<pre>utils.plotHDdata(X[0],Y[0])
plt.show()
</pre>
Several sources can be perceived, however, there is a significant level of noise. Here again, we will use CCA to find subspaces where the two modalities are maximally correlated. In this example, because there are many more dimensions than there are data points, it is more advantageous to solve CCA in thedual.YourtaskistoimplementaCCAdualsolverthatreceivestwodatamatricesofsize N $\times$ d1 and N $\times$ d2 respectivelyasinput, andreturnstheassociateCCAdirections(twovectorsofrespectivesizes d1 and d2).

</div>
</div>
</div>
</div>
<div class="page" title="Page 4">
<div class="section">
<div class="layoutArea">
<div class="column">
In [6]:

def CCAdual(X,Y):

## ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî- ## TODO: replace by your solution ## ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî- import solution

<pre>    wx,wy = solution.CCAdual(X,Y)
</pre>
<pre>    ## -------------------------------
</pre>
return wx,wy

We now call the function we have implemented with a training sequence of 100 pairs of images. Because the returned solution is of same dimensions as

the inputs, it can be rendered in a similar fashion.

<pre>In [7]:
wx,wy = CCAdual(X[:100],Y[:100])
</pre>
<pre>utils.plotHDdata(wx,wy)
plt.show()
</pre>
Here, we can clearly see a common factor that has been extracted between the two fields, specifically a point source emitting at a particular frequency. A test sequence of 100 pairs of images can now be projected on these two filters:

In [8]:

<pre>plt.figure(figsize=(6,2))
plt.plot(numpy.dot(X[100:],wx))
plt.plot(numpy.dot(Y[100:],wy))
plt.show()
</pre>
Clearly the two projected signals are correlated and the input noise has been strongly reduced.

</div>
</div>
</div>
</div>
<div class="page" title="Page 5"></div>
<div class="page" title="Page 6"></div>
<div class="page" title="Page 7"></div>
<div class="page" title="Page 8"></div>
